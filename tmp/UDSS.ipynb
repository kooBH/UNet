{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5d4e2d9-77b0-4395-a4d1-5d3af28cdd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNet based Directional Source Separation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os,sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "try : \n",
    "    from .UNet_m import ComplexConv2d, ComplexConvTranspose2d, ComplexBatchNorm2d\n",
    "except ImportError:\n",
    "    from UNet_m import ComplexConv2d, ComplexConvTranspose2d, ComplexBatchNorm2d\n",
    "\n",
    "class FGRUBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, out_channels):\n",
    "        super(FGRUBlock, self).__init__()\n",
    "        self.GRU = nn.GRU(\n",
    "            in_channels*2, hidden_size*2, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        # the GRU is bidirectional -> multiply hidden_size by 2\n",
    "        self.conv = ComplexConv2d(hidden_size * 2, out_channels, kernel_size=1)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relu = nn.PReLU()\n",
    "\n",
    "    # x : torch.Size([B, C=128, F=2, T=16, RI=2])\n",
    "    def forward(self, x):\n",
    "        B, C, F, T, _ = x.shape\n",
    "        x_ = x.permute(0, 3, 2, 1,4)  # x_.shape == (B,T,F,C,2)\n",
    "        x_ = x_.reshape(B * T, F, C*2)\n",
    "        y, h = self.GRU(x_)  # x_.shape == (BT,F,C*2)\n",
    "        y = y.reshape(B, T, F, self.hidden_size*2,2)\n",
    "        output = y.permute(0, 3, 2, 1, 4)  # output.shape == (B,C,F,T,2)\n",
    "        output = self.conv(output)\n",
    "        output = self.bn(output)\n",
    "        return self.relu(output)\n",
    "    \n",
    "class TGRUBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, out_channels, skipGRU=False,**kwargs):\n",
    "        super(TGRUBlock, self).__init__()\n",
    "\n",
    "        if not skipGRU : \n",
    "            self.GRU = nn.GRU(in_channels*2, hidden_size*2, batch_first=True)\n",
    "        else : \n",
    "            self.GRU = SkipGRU(in_channels*2, hidden_size*2, batch_first=True)\n",
    "        self.conv = ComplexConv2d(hidden_size, out_channels, kernel_size=1)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relu = nn.PReLU()\n",
    "\n",
    "    # x : torch.Size([B, C=128, F=2, T=16, RI=2])\n",
    "    def forward(self, x, rnn_state=None):\n",
    "        B, C, F, T, _ = x.shape  # x.shape == (B, C, T, F)\n",
    "\n",
    "        # unpack, permute, and repack\n",
    "        x1 = x.permute(0, 2, 3, 1, 4)  # x2.shape == (B,F,T,C,2)\n",
    "        x_ = x1.reshape(B * F, T, C*2)  # x_.shape == (BF,T,C*2)\n",
    "\n",
    "        # run GRU\n",
    "        y_, rnn_state = self.GRU(x_, rnn_state)  # y_.shape == (BF,T,C*2)\n",
    "        # unpack, permute, and repack\n",
    "        y1 = y_.reshape(B, F, T, self.hidden_size,2)  # y1.shape == (B,F,T,C,2)\n",
    "        y2 = y1.permute(0, 3, 1, 2, 4)  # y2.shape == (B,C,F,T,2)\n",
    "\n",
    "        output = self.conv(y2)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "    \n",
    "class FTGRUBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, out_channels,**kwargs):\n",
    "        super(FTGRUBlock, self).__init__()\n",
    "        \n",
    "        self.FGRU = FGRUBlock(in_channels, hidden_size, out_channels,**kwargs)\n",
    "        self.TGRU = TGRUBlock(in_channels, hidden_size, out_channels,**kwargs)\n",
    "    \n",
    "    def forward(self, x, rnn_state = None) : \n",
    "        x = self.FGRU(x)\n",
    "        x,rnn_state = self.TGRU(x,rnn_state)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class permuteTF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(permuteTF, self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        if len(x.shape) == 3 :\n",
    "            x = torch.permute(x,(0,2,1))\n",
    "        elif len(x.shape) == 4 :\n",
    "            x = torch.permute(x,(0,1,3,2))\n",
    "        return x\n",
    "    \n",
    "class cRNN(nn.Module) : \n",
    "    def __init__(self,dim,\n",
    "                 style=\"GRU\"\n",
    "                 ):\n",
    "        super(cRNN, self).__init__()\n",
    "    \n",
    "        if style == \"GRU\" : \n",
    "            self.rr = nn.GRU(dim,dim,batch_first=True)\n",
    "            self.ri = nn.GRU(dim,dim,batch_first=True)\n",
    "            self.ir = nn.GRU(dim,dim,batch_first=True)\n",
    "            self.ii = nn.GRU(dim,dim,batch_first=True)\n",
    "        elif style == \"LSTM\" : \n",
    "            self.rr = nn.LSTM(dim,dim,batch_first=True)\n",
    "            self.ri = nn.LSTM(dim,dim,batch_first=True)\n",
    "            self.ir = nn.LSTM(dim,dim,batch_first=True)\n",
    "            self.ii = nn.LSTM(dim,dim,batch_first=True)\n",
    "    \n",
    "        self.re_norm = nn.BatchNorm1d(dim)\n",
    "        self.im_norm = nn.BatchNorm1d(dim)\n",
    "        self.re_activation = nn.PReLU()\n",
    "        self.im_activation = nn.PReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x : [B,C,F,T,2] -> [2,B,T,C*F]\n",
    "        B,C,F,T,_ = x.shape\n",
    "        x = torch.permute(x,(4,0,3,1,2))\n",
    "        x = torch.reshape(x,(2,B,T,C*F))\n",
    "        \n",
    "        rr = self.rr(x[0])[0]\n",
    "        ri = self.ri(x[0])[0]\n",
    "        ir = self.ir(x[1])[0]\n",
    "        ii = self.ii(x[1])[0]\n",
    "        \n",
    "        re = rr - ii\n",
    "        im = ri + ir\n",
    "        \n",
    "        # re : [B,T,C*F]\n",
    "        # im = [B,T,C*F]\n",
    "        \n",
    "        re = self.re_norm(torch.permute(re,(0,2,1)))\n",
    "        im = self.im_norm(torch.permute(im,(0,2,1)))\n",
    "        \n",
    "        re = self.re_activation(re)\n",
    "        im = self.im_activation(im)\n",
    "        \n",
    "        # x  : [2,B,T,C*F] -> [B,C*F,T,2] -> [B,C,F,T,2]\n",
    "        x = torch.stack((re,im),dim=-1)\n",
    "        x = torch.permute(x,(1,3,2,0))\n",
    "        x = torch.reshape(x,(B,C,F,T,2))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_in = 257,\n",
    "                 n_dim=1024,\n",
    "                 n_hidden_layer=1,\n",
    "                 n_out = 40,\n",
    "                 type_activation=None,\n",
    "                 type_normalization=None,\n",
    "                 dropout = 0.0\n",
    "                 ):\n",
    "        super(Attractor, self).__init__()\n",
    "        n_dim = 1024\n",
    "\n",
    "        if type_activation == \"PReLU\" : \n",
    "            activation = nn.PReLU\n",
    "        elif type_activation == \"ReLU\" : \n",
    "            activation = nn.ReLU\n",
    "        else :\n",
    "            activation = nn.Identity\n",
    "\n",
    "        if type_normalization == \"BatchNorm\":\n",
    "            normalization = nn.BatchNorm1d\n",
    "        else  :\n",
    "            normalization = nn.Identity\n",
    "\n",
    "        self.encoder = []\n",
    "        self.acitvation = []\n",
    "        self.normalization = []\n",
    "\n",
    "        self.layers = []\n",
    "        module = nn.Sequential(\n",
    "            nn.Linear(n_in,n_dim),\n",
    "            activation(),\n",
    "            permuteTF(),\n",
    "            normalization(n_dim),\n",
    "            permuteTF()\n",
    "        )\n",
    "        self.layers.append(module)\n",
    "\n",
    "        for i in range(n_hidden_layer):\n",
    "            module = nn.Sequential(\n",
    "                nn.Linear(n_dim,n_dim),\n",
    "                activation(),\n",
    "                permuteTF(),\n",
    "                normalization(n_dim),\n",
    "                permuteTF()\n",
    "            )\n",
    "            self.layers.append(module)\n",
    "\n",
    "        module = nn.Sequential(\n",
    "                nn.Linear(n_dim,n_out),\n",
    "                activation(),\n",
    "                permuteTF(),\n",
    "                normalization(n_out),\n",
    "                permuteTF()\n",
    "            )\n",
    "        self.layers.append(module)\n",
    "\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.DR = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(len(self.layers)) :\n",
    "            x = self.layers[i](x)\n",
    "            x = self.DR(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=None, complex=False, padding_mode=\"zeros\"):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]  # 'SAME' padding\n",
    "            \n",
    "        if complex:\n",
    "            conv = ComplexConv2d\n",
    "            bn = ComplexBatchNorm2d\n",
    "        else:\n",
    "            conv = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "\n",
    "        self.conv = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode=padding_mode)\n",
    "        self.bn = bn(out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding,padding=(0, 0), complex=False):\n",
    "        super().__init__()\n",
    "        if complex:\n",
    "            tconv = ComplexConvTranspose2d\n",
    "            bn = ComplexBatchNorm2d\n",
    "        else:\n",
    "            tconv = nn.ConvTranspose2d\n",
    "            bn = nn.BatchNorm2d\n",
    "        \n",
    "        self.transconv = tconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, output_padding=output_padding,padding=padding)\n",
    "        self.bn = bn(out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UDSS(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_channels=4,\n",
    "                 n_fft=512,\n",
    "                 complex=True,\n",
    "                 #model_complexity=45,\n",
    "                 model_complexity=45,\n",
    "                 bottleneck=\"None\",\n",
    "                 padding_mode=\"zeros\",\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.complex = complex\n",
    "        self.n_channel = input_channels\n",
    "        n_angle = 2\n",
    "\n",
    "        if not complex:\n",
    "            input_channels *=2\n",
    "        else  :\n",
    "            model_complexity = int(model_complexity // 1.414)\n",
    "\n",
    "        model_depth=20\n",
    "\n",
    "        self.set_size(model_complexity=model_complexity, input_channels=input_channels, model_depth=model_depth)\n",
    "        self.model_length = model_depth // 2\n",
    "        self.dropout = dropout\n",
    "\n",
    "        ## Encoder\n",
    "        self.encoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(self.enc_channels[i], self.enc_channels[i + 1], kernel_size=self.enc_kernel_sizes[i],\n",
    "                             stride=self.enc_strides[i], padding=self.enc_paddings[i], complex=complex, padding_mode=padding_mode)\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        ## Decoder\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            module = Decoder(self.dec_channels[i] + self.enc_channels[self.model_length - i], self.dec_channels[i + 1], kernel_size=self.dec_kernel_sizes[i],\n",
    "                             stride=self.dec_strides[i], padding=self.dec_paddings[i], output_padding=self.dec_output_paddings[i],complex=complex)\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "\n",
    "        # Bottleneck\n",
    "        if bottleneck == \"cRNN\" : \n",
    "            self.RNN = cRNN(128*2)\n",
    "        elif bottleneck == \"TGRU\":\n",
    "            self.RNN = TGRUBlock(128,256,128)\n",
    "        elif bottleneck == \"FTGRU\" : \n",
    "            self.RNN = FTGRUBlock(128,256,128)\n",
    "        else :\n",
    "            self.RNN = nn.Identity()\n",
    "            \n",
    "        ## Attractor\n",
    "        self.attractors =  [] \n",
    "\n",
    "        for i in range(self.model_length-1) : \n",
    "            module = Attractor(\n",
    "                n_in=n_angle,\n",
    "                n_dim=257,\n",
    "                n_out = self.enc_channels[i+1]\n",
    "            )\n",
    "            self.add_module(\"attractor{}\".format(i),module)\n",
    "            self.attractors.append(module)\n",
    "        self.bottleneck_attactor = Attractor(\n",
    "            n_in = n_angle,\n",
    "            n_dim =257,\n",
    "            n_out = self.enc_channels[-1]\n",
    "        )\n",
    "\n",
    "        if complex:\n",
    "            conv = ComplexConv2d\n",
    "            linear = conv(self.dec_channels[-1], 1, 1)\n",
    "        else:\n",
    "            conv = nn.Conv2d\n",
    "            linear = conv(self.dec_channels[-1], 2, 1)\n",
    "\n",
    "        ## Mask Estimator\n",
    "        self.add_module(\"linear\", linear)\n",
    "        self.complex = complex\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "        self.dr = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        self.attractors = nn.ModuleList(self.attractors)\n",
    "\n",
    "    def forward(self, sf,af):        \n",
    "        # ipnut : [ Batch Channel Freq Time 2]\n",
    "\n",
    "        # Encoders\n",
    "        sf_skip = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            sf_skip.append(sf)\n",
    "            sf = encoder(sf)\n",
    "            sf = self.dr(sf)\n",
    "            #print(\"sf{}\".format(i), sf.shape)\n",
    "        # sf_skip : sf0=input sf1 ... sf9\n",
    "\n",
    "        p = sf\n",
    "    \n",
    "        p = self.RNN(p)\n",
    "\n",
    "        # Bottleneck\n",
    "        a_s = self.bottleneck_attactor(af)\n",
    "        a_s = torch.reshape(a_s,(*a_s.shape,1,1,1))\n",
    "        p = a_s*p\n",
    "\n",
    "        # Attractor - Skip\n",
    "        for i,attractor in enumerate(self.attractors) : \n",
    "            a_s = attractor(af)\n",
    "            a_s = torch.reshape(a_s,(*a_s.shape,1,1,1))\n",
    "            sf_skip[i+1] = a_s*sf_skip[i+1]\n",
    "            #print(\"attractor[{}] : {}*{}\".format(i,a_s.shape,sf_skip[i+1].shape))\n",
    "        \n",
    "        # Decoders\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            #print(f\"p{i}, {p.shape} + sf{self.model_length - 1 - i}, {sf_skip[self.model_length - 1 -i].shape}, padding {self.dec_paddings[i]}\")\n",
    "            \n",
    "            p = torch.cat([p, sf_skip[self.model_length - 1 - i]], dim=1)\n",
    "\n",
    "        #:print(p.shape)\n",
    "        mask = self.linear(p)\n",
    "        mask = torch.tanh(mask)\n",
    "        mask = torch.squeeze(mask,1)\n",
    "        mask = mask[...,0] + 1j*mask[...,1]\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "        self.enc_channels = [input_channels,\n",
    "                                model_complexity,\n",
    "                                model_complexity,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                128]\n",
    "\n",
    "        self.enc_kernel_sizes = [(7, 1),\n",
    "                                    (1, 7),\n",
    "                                    (7, 5),\n",
    "                                    (7, 5),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3)]\n",
    "\n",
    "        self.enc_strides = [(1, 1),\n",
    "                            (1, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1)]\n",
    "\n",
    "        self.enc_paddings = [(3, 0),\n",
    "                                (0, 3),\n",
    "                                (3, 2),\n",
    "                                (3, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),]\n",
    "                            \n",
    "                                \n",
    "\n",
    "        self.dec_channels = [0,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2,\n",
    "                                model_complexity * 2]\n",
    "\n",
    "        self.dec_kernel_sizes = [(5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3),\n",
    "                                    (5, 3), \n",
    "                                    (7, 5), \n",
    "                                    (7, 5), \n",
    "                                    (1, 7),\n",
    "                                    (7, 1)]\n",
    "\n",
    "        self.dec_strides = [(2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (2, 1),\n",
    "                            (2, 2),\n",
    "                            (1, 1),\n",
    "                            (1, 1)]\n",
    "\n",
    "        self.dec_paddings = [(2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (2, 1),\n",
    "                                (3, 2),\n",
    "                                (3, 2),\n",
    "                                (0, 3),\n",
    "                                (3, 0)]\n",
    "        self.dec_output_paddings = [(0,0),\n",
    "                                    (0,1),\n",
    "                                    (0,0),\n",
    "                                    (0,1),\n",
    "                                    (0,0),\n",
    "                                    (0,1),\n",
    "                                    (0,0),\n",
    "                                    (0,1),\n",
    "                                    (0,0),\n",
    "                                    (0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01c70376-570a-4ee0-b757-3ae4684d51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : torch.Size([2, 257, 256])\n"
     ]
    }
   ],
   "source": [
    "B = 2\n",
    "C = 4\n",
    "F = 257\n",
    "L = 32000\n",
    "T = 256\n",
    "\n",
    "#x = torch.rand(B,C,128*127)\n",
    "sf = torch.rand(B,C*2,F,T,2)\n",
    "af = torch.rand(B,2)\n",
    "m = UDSS(input_channels=8, bottleneck = \"FTGRU\")\n",
    "\n",
    "y = m(sf,af)\n",
    "\n",
    "print(\"output : {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44338053-a75b-4fcb-b3d0-6af6a9ee101d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
